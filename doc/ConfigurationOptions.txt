# This is an auto-generated file, DO NOT EDIT!
# Run ant to generate it.

# Possible log levels in descending order 
# (lower levels include higher ones):
# OFF:      no logs published
# SEVERE:   error messages
# WARNING:  warnings
# INFO:     messages
# FINE:     logs on main application level
# FINER:    logs on central CPA algorithm level
# FINEST:   logs published by specific CPAs
# ALL:      debugging information
# Care must be taken with levels of FINER or lower, as output files may
# become quite large and memory usage might become an issue.

# single levels to be excluded from being logged
log.consoleExclude = ImmutableList.of()

# log level of console output
log.consoleLevel = Level.INFO

# name of the log file
log.file = "CPALog.txt"

# single levels to be excluded from being logged
log.fileExclude = ImmutableList.of()

# log level of file output
log.level = Level.OFF

# maximum size of log output strings before they will be truncated
log.truncateSize = 10000

# use colors for log messages on console
log.useColors = true

# all used options are printed
log.usedOptions.export = false

# disable all default output files
# (any explicitly given file will still be written)
output.disable = false

# directory to put all output files in
output.path = "output/"

# base directory for all input & output files
# (except for the configuration file itself)
rootDirectory = "."


# maximum number of condition adjustments (-1 for infinite)
adjustableconditions.adjustmentLimit = -1

# stop CPAchecker after startup (internal option, not intended for users)
analysis.disable = false

# entry function
analysis.entryFunction = "main"

# use CBMC as an external tool from CPAchecker
analysis.externalCBMC = false

# run interprocedural analysis
analysis.interprocedural = true

# the machine model, which determines the sizes of types like int
analysis.machineModel = LINUX32
  enum:     [LINUX32, LINUX64]

# C programs to analyze (currently only one file is supported)
analysis.programNames = no default value

# which reached set implementation to use?
# NORMAL: just a simple set
# LOCATIONMAPPED: a different set per location (faster, states with different
# locations cannot be merged)
# PARTITIONED: partitioning depending on CPAs (e.g Location, Callstack etc.)
analysis.reachedSet = PARTITIONED
  enum:     [NORMAL, LOCATIONMAPPED, PARTITIONED]

# restart the algorithm using a different CPA after unknown result
analysis.restartAfterUnknown = false

# stop after the first error has been found
analysis.stopAfterError = true

# which strategy to adopt for visiting states?
analysis.traversal.order = DFS
  enum:     [DFS, BFS, RAND]

# handle states with a deeper callstack first?
# This needs the CallstackCPA to have any effect.
analysis.traversal.useCallstack = false

# handle more abstract states (with less information) first? (only for
# ExplicitCPA)
analysis.traversal.useExplicitInformation = false

# Use an implementation of reverse postorder strategy that allows to select a
# secondary strategy that is used if there are two states with the same
# reverse postorder id. The secondary strategy is selected with
# 'analysis.traversal.order'.
analysis.traversal.useReversePostorder = false

# DEPRECATED: This option was renamed to
# analysis.traversal.useReversePostorder and will soon get removed.
analysis.traversal.useTopsort = false

# use adjustable conditions algorithm
analysis.useAdjustableConditions = false

# use assumption collecting algorithm
analysis.useAssumptionCollector = false

# use CBMC and the BDDCPA Restriction option
analysis.useBDDCPARestriction = false

# use a BMC like algorithm that checks for satisfiability after the analysis
# has finished, works only with PredicateCPA
analysis.useBMC = false

# use CBMC to double-check counter-examples
analysis.useCBMC = false

# use CBMC and the FeatureVars Restriction option
analysis.useFeatureVarsRestriction = false

# add declarations for global variables before entry function
analysis.useGlobalVars = true

# Use McMillan's Impact algorithm for lazy interpolation
analysis.useImpactAlgorithm = false

# use a proof check algorithm to validate a previously generated proof
analysis.useProofCheckAlgorithm = false

# use CEGAR algorithm for lazy counter-example guided analysis
# You need to specify a refiner with the cegar.refiner option.
# Currently all refiner require the use of the ARGCPA.
analysis.useRefinement = false

# Add a threshold to the automaton, after so many branches on a path the
# automaton will be ignored (0 to disable)
assumptions.automatonBranchingThreshold = 0

# write collected assumptions as automaton to file
assumptions.automatonFile = "AssumptionAutomaton.txt"

# write collected assumptions to file
assumptions.export = true
assumptions.file = "assumptions.txt"

# Initial size of the BDD cache.
bdd.initBddCacheSize = 1000

# Initial size of the BDD node table.
bdd.initBddNodeTableSize = 10000

# Which BDD package should be used?
# - java:  JavaBDD (default, no dependencies, many features)
# - cudd:  CUDD (native library required, reordering not supported)
# - micro: MicroFactory (maximum number of BDD variables is 1024, slow, but
# less memory-comsumption)
# - buddy: Buddy (native library required)
# - cal:   CAL (native library required)
# - jdd:   JDD
bdd.package = "java"
  allowed values: [java, cudd, micro, buddy, cal, jdd]

# Allow reduction of function entries; calculate abstractions alwasy at
# function entries?
blockreducer.allowReduceFunctionEntries = true

# Allow reduction of function exits; calculate abstractions alwasy at
# function exits?
blockreducer.allowReduceFunctionExits = true

# Allow reduction of loop heads; calculate abstractions alwasy at loop heads?
blockreducer.allowReduceLoopHeads = false

# write the reduced cfa to the specified file.
blockreducer.reducedCfaFile = "ReducedCfa.rsf"

# Do at most n summarizations on a node.
blockreducer.reductionThreshold = 100

# If BMC did not find a bug, check whether the bounding did actually remove
# parts of the state space (this is similar to CBMC's unwinding assertions).
bmc.boundingAssertions = true

# Check reachability of target states after analysis (classical BMC). The
# alternative is to check the reachability as soon as the target states are
# discovered, which is done if cpa.predicate.targetStateSatCheck=true.
bmc.checkTargetStates = true

# dump counterexample formula to file
bmc.dumpCounterexampleFormula = "counterexample.msat"

# try using induction to verify programs with loops
bmc.induction = false

# configuration file for invariant generation
bmc.invariantGenerationConfigFile = no default value

# generate invariants for induction in parallel to the analysis
bmc.parallelInvariantGeneration = false

# file name where to put the path program that is generated as input for
# CBMC. A temporary file is used if this is unspecified.
cbmc.dumpCBMCfile = no default value

# specify the name of the error label
cbmc.options.errorLabel = "ERROR"

# set width of int (16, 32 or 64)
cbmc.options.intWidth = 32

# disable unwinding assertions violation error
cbmc.options.nuaf = false

# specify the limit for unwindings (0 is infinite)
cbmc.options.unwindings = 0

# maximum time limit for CBMC (use milliseconds or specify a unit; 0 for
# infinite)
# maximum time limit for CBMC (0 is infinite)
cbmc.timelimit = 0

# Whether to do refinement immediately after finding an error state, or
# globally after the ARG has been unrolled completely.
cegar.globalRefinement = false

# Which refinement algorithm to use? (give class name, required for CEGAR) If
# the package name starts with 'org.sosy_lab.cpachecker.', this prefix can be
# omitted.
cegar.refiner = no default value

# export CFA as .dot file
cfa.export = true

# export individual CFAs for function as .dot files
cfa.exportPerFunction = true

# export CFA as .dot file
cfa.file = "cfa.dot"

# remove paths from CFA that cannot lead to a specification violation
cfa.removeIrrelevantForSpecification = false

# combine sequences of simple edges into a single edge
cfa.useMultiEdges = false

# Dump variable classification to a file.
cfa.variableClassification.logfile = "VariableClassification.log"

# Dump the complete configuration to a file.
configuration.dumpFile = "UsedConfiguration.properties"

# which model checker to use for verifying counterexamples as a second check
# Currently CBMC or CPAchecker with a different config can be used.
counterexample.checker = "CBMC"
  allowed values: [CBMC, CPACHECKER]

# configuration file for counterexample checks with CPAchecker
counterexample.checker.config = "config/explicitAnalysis-no-cbmc.properties"

# continue analysis after an counterexample was found that was denied by the
# second check
counterexample.continueAfterInfeasibleError = true

# If continueAfterInfeasibleError is true, remove the infeasible
# counterexample before continuing.Setting this to false may prevent a lot of
# similar infeasible counterexamples to get discovered, but is unsound
counterexample.removeInfeasibleErrors = false

# CPA to use (see doc/Configuration.txt for more documentation on this)
cpa = CompositeCPA.class.getCanonicalName()

# if enabled, cache queries also consider blocks with non-matching precision
# for reuse.
cpa.abm.aggressiveCaching = true

# Type of partitioning (FunctionAndLoopPartitioning or
# DelayedFunctionAndLoopPartitioning)
# or any class that implements a PartitioningHeuristic
cpa.abm.blockHeuristic = FunctionAndLoopPartitioning.class

# if enabled, the reached set cache is analysed for each cache miss to find
# the cause of the miss.
cpa.abm.gatherCacheMissStatistics = false

# export one variable assignment for error path to file, if one is found
cpa.arg.errorPath.assignment = "ErrorPathAssignment.txt"

# export error path to file, if one is found
cpa.arg.errorPath.core = "ErrorPathCore.txt"
cpa.arg.errorPath.export = true
cpa.arg.errorPath.file = "ErrorPath.txt"
cpa.arg.errorPath.graph = "ErrorPath.dot"
cpa.arg.errorPath.json = "ErrorPath.json"
cpa.arg.errorPath.source = "ErrorPath.c"

# translate Error Path to C File 
cpa.arg.errorPath.toCFile = true

# export final ARG as .dot file
cpa.arg.export = true
cpa.arg.file = "ARG.dot"

# whether to keep covered states in the reached set as addition to keeping
# them in the ARG
cpa.arg.keepCoveredStatesInReached = false

# signal the analysis to break in case of reached error state
cpa.automaton.breakOnTargetState = true

# export automaton to file
cpa.automaton.dotExport = false

# file for saving the automaton in DOT format (%s will be replaced with
# automaton name)
cpa.automaton.dotExportFile = "%s.dot"

# file with automaton specification for ObserverAutomatonCPA and
# ControlAutomatonCPA
cpa.automaton.inputFile = no default value

# adding a new value to the state can be done from 0 to N or from N to 0
cpa.bdd.addIncreasing = false

# default bitsize for values and vars
cpa.bdd.bitsize = 32

# use a smaller bitsize for all vars, that have only intEqual values
cpa.bdd.compressIntEqual = true

# Pattern for variablenames that will always be tracked with BDDs.This
# pattern should only be used for known variables, i.e. for boolean vars.
cpa.bdd.forceTrackingPattern = ""

# initialize all variables to 0 when they are declared
cpa.bdd.initAllVars = false

# declare the bits of a var from 0 to N or from N to 0
cpa.bdd.initBitsIncreasing = true

# declare first bit of all vars, then second bit,...
cpa.bdd.initBitwise = true

# declare vars partitionwise
cpa.bdd.initPartitions = true

# declare partitions ordered
cpa.bdd.initPartitionsOrdered = true

# track boolean variables from cfa
cpa.bdd.trackBoolean = true

# track variables, only used in simple calculations (add, sub, gt, lt,
# eq,...) from cfa as bitvectors with (default) 32 bits
cpa.bdd.trackIntAdd = true

# track variables from cfa, that are only compared for equality, they are
# tracked as (small) bitvectors
cpa.bdd.trackIntEqual = true

# which composite merge operator to use (plain or agree)
# Both delegate to the component cpas, but agree only allows merging if all
# cpas agree on this. This is probably what you want.
cpa.composite.merge = "AGREE"
  allowed values: [PLAIN, AGREE]

# which precision adjustment strategy to use (ignorant or omniscient)
# While an ignorant strategy keeps the domain knowledge seperated, and
# delegates to the component precision adjustment operators, the omniscient
# strategy may operate on global knowledge.
cpa.composite.precAdjust = "IGNORANT"
  allowed values: [IGNORANT, OMNISCIENT]

# Limit for Java heap memory used by CPAchecker (in MiB; -1 for infinite)
cpa.conditions.global.memory.heap = -1

# Limit for process memory used by CPAchecker (in MiB; -1 for infinite)
cpa.conditions.global.memory.process = -1

# Limit for size of reached set (-1 for infinite)
cpa.conditions.global.reached.size = -1

# Limit for cpu time used by CPAchecker (use milliseconds or specify a unit;
# -1 for infinite)
cpa.conditions.global.time.cpu = -1

# Hard limit for cpu time used by CPAchecker (use milliseconds or specify a
# unit; -1 for infinite)
# When using adjustable conditions, analysis will end after this threshold
cpa.conditions.global.time.cpu.hardlimit = -1

# Limit for wall time used by CPAchecker (use milliseconds or specify a unit;
# -1 for infinite)
cpa.conditions.global.time.wall = -1

# Hard limit for wall time used by CPAchecker (use milliseconds or specify a
# unit; -1 for infinite)
# When using adjustable conditions, analysis will end after this threshold
cpa.conditions.global.time.wall.hardlimit = -1

# whether or not to track unique assignments only
cpa.conditions.path.assignments.demandUniqueness = true

# file name where to put the extended stats file
cpa.conditions.path.assignments.extendedStatsFile = no default value

# maximum number of assignments (-1 for infinite)
cpa.conditions.path.assignments.threshold = -1

# maximum number of assume edges length (-1 for infinite)
cpa.conditions.path.assumeedges.limit = -1

# The condition
cpa.conditions.path.condition = no default value

# maximum path length (-1 for infinite)
cpa.conditions.path.length.limit = -1

# maximum repetitions of any edge in a path (-1 for infinite)
cpa.conditions.path.repetitions.limit = -1

# which merge operator to use for DefUseCPA
cpa.defuse.merge = "sep"
  allowed values: [sep, join]

# if there is an assumption like (x!=0), this option sets unknown
# (uninitialized) variables to 1L, when the true-branch is handled.
cpa.explicit.initAssumptionVars = false

# which merge operator to use for ExplicitCPA
cpa.explicit.merge = "SEP"
  allowed values: [SEP, JOIN]

# ignore boolean variables. if this option is used, booleans from the cfa
# should tracked with another CPA, i.e. with BDDCPA.
cpa.explicit.precision.ignoreBoolean = false

# ignore variables, that are only used in simple calculations (add, sub, lt,
# gt, eq). if this option is used, these variables from the cfa should
# tracked with another CPA, i.e. with BDDCPA.
cpa.explicit.precision.ignoreIntAdd = false

# ignore variables, that are only compared for equality. if this option is
# used, these variables from the cfa should tracked with another CPA, i.e.
# with BDDCPA.
cpa.explicit.precision.ignoreIntEqual = false

# threshold for amount of different values that are tracked for one variable
# per path (-1 means infinitely)
cpa.explicit.precision.path.defaultThreshold = -1

# threshold for amount of different values that are tracked for one variable
# within the reached set (-1 means infinitely)
cpa.explicit.precision.reachedSet.defaultThreshold = -1

# whether or not to add newly-found variables only to the exact program
# location or to the whole scope of the variable.
cpa.explicit.precision.refinement.useScopedInterpolation = false

# whether or not to only abstract variables that where updated in the
# foregoing post operation
cpa.explicit.precisionAdjustment.useDeltaPrecision = false

# whether or not to always use the inital node as starting point for the next
# re-exploration of the ARG
cpa.explicit.refiner.useInitialNodeAsRestartingPoint = true

# which stop operator to use for ExplicitCPA
cpa.explicit.stop = "SEP"
  allowed values: [SEP, JOIN, NEVER]

# blacklist regex for variables that won't be tracked by ExplicitCPA
cpa.explicit.variableBlacklist = ""

# whitelist regex for variables that will be tracked by FeatureVarsCPA
cpa.featurevars.variableWhitelist = ""

# Which strategy to use for forced coverings (empty for none)
cpa.forcedCovering = no default value

# whether function pointers with invalid targets (e.g., 0) should be tracked
# in order to find calls to such pointers
cpa.functionpointer.trackInvalidFunctionPointers = false

# which type of merge operator to use for IntervalAnalysisCPA
cpa.interval.merge = "SEP"
  allowed values: [SEP, JOIN]

# decides whether one (false) or two (true) successors should be created when
# an inequality-check is encountered
cpa.interval.splitIntervals = false

# at most that many intervals will be tracked per variable
cpa.interval.threshold = 0

# which merge operator to use for InvariantCPA
cpa.invariants.merge = "JOIN"
  allowed values: [JOIN, SEP]

# threshold for unrolling loops of the program (0 is infinite)
# works only if assumption storage CPA is enabled, because otherwise it would
# be unsound
cpa.loopstack.maxLoopIterations = 0

# time limit for a single post computation (use milliseconds or specify a
# unit; 0 for infinite)
cpa.monitor.limit = 0

# time limit for all computations on a path in milliseconds (use milliseconds
# or specify a unit; 0 for infinite)
cpa.monitor.pathcomputationlimit = 0

# which merge operator to use for OctagonCPA?
cpa.octagon.merge = "SEP"
  allowed values: [SEP, JOIN]

# which merge operator to use for PointerCPA?
cpa.pointer.merge = "sep"
  allowed values: [sep, join]

# print warnings during analysis when unsafe pointer operations are found
cpa.pointer.printWarnings = true

# which merge operator to use for PointerACPA
cpa.pointerA.merge = "SEP"
  allowed values: [SEP, JOIN]

# which stop operator to use for PointerACPA
cpa.pointerA.stop = "SEP"
  allowed values: [SEP, JOIN, NEVER]

# whether to use auxiliary predidates for reduction
cpa.predicate.abm.auxiliaryPredicateComputer = true

# use caching of region to formula conversions
# use caching of abstractions
cpa.predicate.abs.useCache = true

# whether to use Boolean (false) or Cartesian (true) abstraction
cpa.predicate.abstraction.cartesian = false

# dump the abstraction formulas if they took to long
cpa.predicate.abstraction.dumpHardQueries = false

# get an initial map of predicates from a file (see source
# doc/examples/predmap.txt for an example)
cpa.predicate.abstraction.initialPredicates = no default value

# What to use for storing abstractions
cpa.predicate.abstraction.type = "BDD"
  allowed values: [BDD, FORMULA]

# force abstractions immediately after threshold is reached (no effect if
# threshold = 0)
cpa.predicate.blk.alwaysAfterThreshold = true

# abstraction always and only on explicitly computed abstraction nodes.
cpa.predicate.blk.alwaysAndOnlyAtExplicitNodes = false

# force abstractions at each function calls/returns, regardless of threshold
cpa.predicate.blk.alwaysAtFunctions = true

# force abstractions at loop heads, regardless of threshold
cpa.predicate.blk.alwaysAtLoops = true

# abstractions at function calls/returns if threshold has been reached (no
# effect if threshold = 0)
cpa.predicate.blk.functions = false

# abstractions at loop heads if threshold has been reached (no effect if
# threshold = 0)
cpa.predicate.blk.loops = false

# maximum blocksize before abstraction is forced
# (non-negative number, special values: 0 = don't check threshold, 1 = SBE)
cpa.predicate.blk.threshold = 0

# use caching of path formulas
cpa.predicate.blk.useCache = true

# always check satisfiability at end of block, even if precision is empty
cpa.predicate.checkBlockFeasibility = false

# Enable the possibility to precompute explicit abstraction locations.
cpa.predicate.enableBlockreducer = false

# where to dump interpolation and abstraction problems (format string)
cpa.predicate.formulaDumpFilePattern = "%s%04d-%s%03d.msat"

# Handle aliasing of pointers. This adds disjunctions to the formulas, so be
# careful when using cartesian abstraction.
cpa.predicate.handlePointerAliasing = true

# initialize all variables to 0 when they are declared
cpa.predicate.initAllVars = false

# export final loop invariants
cpa.predicate.invariants.export = true

# file for exporting final loop invariants
cpa.predicate.invariants.file = "invariants.txt"

# use uninterpreted functions for *, & and array access
cpa.predicate.lvalsAsUIFs = false

# List of further options which will be passed to Mathsat. Format is
# 'key1=value1,key2=value2'
cpa.predicate.mathsat.furtherOptions = ""

# Export solver queries in Smtlib format into a file (for Mathsat5).
cpa.predicate.mathsat.logAllQueries = false
cpa.predicate.mathsat.logfile = "mathsat5.%d.smt2"

# Use UIFs (recommended because its more precise)
cpa.predicate.mathsat.useUIFs = true

# list of functions that provide new memory on the heap. This is only used,
# when handling of pointers is enabled.
cpa.predicate.memoryAllocationFunctions = {
      "malloc", "__kmalloc", "kzalloc"
      }

# which merge operator to use for predicate cpa (usually ABE should be used)
cpa.predicate.merge = "ABE"
  allowed values: [SEP, ABE]

# list of functions that should be considered as giving a non-deterministic
# return value
#  Only predicate analysis honors this option. If you specify this option,
# the default values are not added automatically to the list, so you need to
# specify them explicitly if you need them. Mentioning a function in this
# list has only an effect, if it is an 'external function', i.e., no source
# is given in the code for this function.
cpa.predicate.nondetFunctions = {
      "malloc", "__kmalloc", "kzalloc",
      "sscanf",
      "random"}

# Regexp pattern for functions that should be considered as giving a
# non-deterministic return value (c.f. cpa.predicate.nondedFunctions)
cpa.predicate.nondetFunctionsRegexp = "^(__VERIFIER_)?nondet_[a-z]*"

# export final predicate map
cpa.predicate.predmap.export = true

# file for exporting final predicate map
cpa.predicate.predmap.file = "predmap.txt"

# refinement will add all discovered predicates to all the locations in the
# abstract trace
cpa.predicate.refinement.addPredicatesGlobally = false

# refinement will try to build 'well-scoped' predicates, by cutting spurious
# traces as explained in Section 5.2 of the paper 'Abstractions From Proofs'
# (this does not work with function inlining).
# THIS FEATURE IS CURRENTLY NOT AVAILABLE. 
cpa.predicate.refinement.addWellScopedPredicates = false

# use only the atoms from the interpolants as predicates, and not the whole
# interpolant
cpa.predicate.refinement.atomicPredicates = true

# dump all interpolation problems
cpa.predicate.refinement.dumpInterpolationProblems = false

# apply deletion-filter to the abstract counterexample, to get a minimal set
# of blocks, before applying interpolation-based refinement
cpa.predicate.refinement.getUsefulBlocks = false

# During refinement, keep predicates from all removed parts of the ARG.
# Otherwise, only predicates from the error path are kept.
cpa.predicate.refinement.keepAllPredicates = false

# skip refinement if input formula is larger than this amount of bytes
# (ignored if 0)
cpa.predicate.refinement.maxRefinementSize = 0

# where to dump the counterexample formula in case the error location is
# reached
cpa.predicate.refinement.msatCexFile = "counterexample.msat"

# Do a complete restart (clearing the reached set) after N refinements. 0 to
# disable, 1 for always.
cpa.predicate.refinement.restartAfterRefinements = 0

# During refinement, add all new predicates to the precisions of all abstract
# states in the reached set.
cpa.predicate.refinement.sharePredicates = false

# use incremental search in counterexample analysis, to find the minimal
# infeasible prefix
cpa.predicate.refinement.shortestCexTrace = false

# if shortestCexTrace is used, start from the end with the incremental search
cpa.predicate.refinement.shortestCexTraceUseSuffix = false

# if shortestCexTrace is used, alternatingly search from start and end of the
# trace
cpa.predicate.refinement.shortestCexTraceZigZag = false

# split each arithmetic equality into two inequalities when extracting
# predicates from interpolants
# split arithmetic equalities when extracting predicates from interpolants
cpa.predicate.refinement.splitItpAtoms = false

# time limit for refinement (use milliseconds or specify a unit; 0 for
# infinite)
cpa.predicate.refinement.timelimit = 0

# verify if the interpolants fulfill the interpolant properties
cpa.predicate.refinement.verifyInterpolants = false

# maximum blocksize before a satisfiability check is done
# (non-negative number, 0 means never, if positive should be smaller than
# blocksize)
cpa.predicate.satCheck = 0

# Export solver queries in Smtlib format into a file.
cpa.predicate.smtinterpol.logAllQueries = false
cpa.predicate.smtinterpol.logfile = "smtinterpol.smt2"

# Whether to use MathSAT 5 or SmtInterpol as SMT solver
cpa.predicate.solver = MATHSAT5
  allowed values: [MATHSAT5, SMTINTERPOL]

# With of the bitvectors if useBitvectors is true.
cpa.predicate.solver.bitWidth = 32

# Whether to use signed or unsigned variables if useBitvectors is true.
cpa.predicate.solver.signed = true

# Encode program variables as bitvectors of a fixed size,instead of using
# REALS. Not all solvers might support this.
cpa.predicate.solver.useBitvectors = false

# Encode program variables as INTEGER variables, instead of using REALs. Not
# all solvers might support this.
cpa.predicate.solver.useIntegers = false

# whether to include the symbolic path formula in the coverage checks or do
# only the fast abstract checks
cpa.predicate.symbolicCoverageCheck = false

# check satisfiability when a target state has been found (should be true)
cpa.predicate.targetStateSatCheck = true

# try to add some useful static-learning-like axioms for bitwise operations
# (which are encoded as UFs): essentially, we simply collect all the numbers
# used in bitwise operations, and add axioms like (0 & n = 0)
cpa.predicate.useBitwiseAxioms = false

# add special information to formulas about non-deterministic functions
cpa.predicate.useNondetFlags = false

# which merge operator to use for UninitializedVariablesCPA?
cpa.uninitvars.merge = "sep"
  allowed values: [sep, join]

# print warnings during analysis when uninitialized variables are used
cpa.uninitvars.printWarnings = "true"

# which stop operator to use for UninitializedVariablesCPA?
cpa.uninitvars.stop = "sep"
  allowed values: [sep, join]

# enable the Forced Covering optimization
impact.useForcedCovering = true

# use Language java with following src Path
java.encoding = "utf8"
java.rootPath = no default value
java.version = JavaCore.VERSION_1_7
language.java = false

# C dialect for parser
parser.dialect = GNUC
  enum:     [C99, GNUC]

# file in which ARG representation needed for proof checking is stored
pcc.proofFile = "arg.obj"

pcc.proofgen.doPCC = false

# print reached set to text file
reachedSet.export = true
reachedSet.file = "reached.txt"

# list of files with configurations to use
restartAlgorithm.configFiles = no default value

# comma-separated list of files with specifications that should be checked
# (see config/specification/ for examples)
specification = no default value

# write some statistics to disk
statistics.export = true
statistics.file = "Statistics.txt"

# track memory usage of JVM during runtime
statistics.memory = true

# print statistics to console
statistics.print = false

